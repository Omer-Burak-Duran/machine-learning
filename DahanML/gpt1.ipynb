{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4335,"status":"ok","timestamp":1719859196069,"user":{"displayName":"Dağhan Erdönmez","userId":"11594417472388186074"},"user_tz":-180},"id":"jWAOol5Ul2dw"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":328,"status":"ok","timestamp":1719859197906,"user":{"displayName":"Dağhan Erdönmez","userId":"11594417472388186074"},"user_tz":-180},"id":"8_aNSXr_mK21","outputId":"b5bef2eb-1d94-4751-c54c-bb673c6ceb68"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":286,"status":"ok","timestamp":1719859282278,"user":{"displayName":"Dağhan Erdönmez","userId":"11594417472388186074"},"user_tz":-180},"id":"saO4sD4AmMCw"},"outputs":[],"source":["# hyperparameters\n","batch_size = 64 # how many independent sequences will we process in parallel?\n","block_size = 128 # what is the maximum context length for predictions?\n","max_iters = 5000\n","eval_interval = 1\n","learning_rate = 3e-4\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","#device = 'cpu'\n","eval_iters = 200\n","n_embd = 256\n","n_head = 4\n","n_layer = 4\n","dropout = 0.2\n","# ------------"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1719859283435,"user":{"displayName":"Dağhan Erdönmez","userId":"11594417472388186074"},"user_tz":-180},"id":"4vdMy7N-mNwY","outputId":"72a85d40-4fc0-467d-8951-ddb9e05c14fb"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x1a3fe95d170>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(1337)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":336,"status":"ok","timestamp":1719859284941,"user":{"displayName":"Dağhan Erdönmez","userId":"11594417472388186074"},"user_tz":-180},"id":"vszcwQ4SmOd2"},"outputs":[],"source":["# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","with open('input.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()\n","\n","# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","# Train and test splits\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1719859287794,"user":{"displayName":"Dağhan Erdönmez","userId":"11594417472388186074"},"user_tz":-180},"id":"tUJyz1sRmPks","outputId":"82d0eed1-4b2b-44d2-f3ac-f4f1436a58b7"},"outputs":[{"data":{"text/plain":["'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["decode(list(char.item() for char in train_data[:100]))"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1719859290835,"user":{"displayName":"Dağhan Erdönmez","userId":"11594417472388186074"},"user_tz":-180},"id":"ilMRSYqomQRd"},"outputs":[],"source":["# data loading\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # input of size (batch, time-step, channels)\n","        # output of size (batch, time-step, head size)\n","        B,T,C = x.shape\n","        k = self.key(x)   # (B,T,hs)\n","        q = self.query(x) # (B,T,hs)\n","        # compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T)\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,hs)\n","        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(head_size * num_heads, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","\n","class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","\n","class GPTLanguageModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","        # better init, not covered in the original GPT video, but important, will cover in followup video\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n","        x = tok_emb + pos_emb # (B,T,C)\n","        x = self.blocks(x) # (B,T,C)\n","        x = self.ln_f(x) # (B,T,C)\n","        logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":292,"status":"ok","timestamp":1719859292338,"user":{"displayName":"Dağhan Erdönmez","userId":"11594417472388186074"},"user_tz":-180},"id":"Wu_LddwBmRZQ","outputId":"d6360d30-b1a5-497d-8199-fbe173251a0c"},"outputs":[{"name":"stdout","output_type":"stream","text":["3.222593 M parameters\n"]}],"source":["model = GPTLanguageModel()\n","m = model.to(device)\n","# print the number of parameters in the model\n","print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1397,"status":"ok","timestamp":1719859294538,"user":{"displayName":"Dağhan Erdönmez","userId":"11594417472388186074"},"user_tz":-180},"id":"mdvV0xA7mTEh"},"outputs":[],"source":["# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xQcKkAirmT0Z","outputId":"eae865f7-530f-4436-9e63-eb7bfd37d888"},"outputs":[{"name":"stdout","output_type":"stream","text":["step 0: train loss 4.2233, val loss 4.2228\n","\n","TKXeuW;pQijkSLWfUaaJK;bcETPs;ocFF&oN Y$eQdJ OEFWggjNhhqv;Es\n","dawL&rqC&Cg'N'PR$mFHUjX.&sumfhnCUzg&AHnNVvN!IebiqGsSEvDt3Zw?Bjx$oYkRwPmDUvRobd de!IKjRUUasbngevEEYEC! 'scf:rl!Eba$Col$CUGf GKtFwssMsJ!.fX?&D?;E$zBP.SudGdAI.UT&\n","qvqNqQrDRR'QGH.s'xq&.hYQW dLUEZE&xNS\n","TIRkftgbNgW\n","E.uJaekBKvfh'S&QW,LDqx'dmcC&Z;xz&GFpgRQeU$y;yDu$vsa,xI&HhepTHJeWjzKdxf?r!Hm?QfwSUvwMMg;fi&$jCXTyow;FZhjVBm$3g33s,ubxKFsjMGaX;p!gUlN,hjDHE. UnUv.HjMCsy-QpwAKylbSHT'pXh3Unfb:mEeo ZrL'bpe-Sq,n\n","q\n","N OpKiGTqUSRs.Ck&zzlGJgybrjDs3fIjOjjFHE\n","step 1: train loss 3.6290, val loss 3.6542\n","step 2: train loss 3.5064, val loss 3.5384\n","step 3: train loss 3.4523, val loss 3.4841\n","step 4: train loss 3.4190, val loss 3.4489\n","step 5: train loss 3.3909, val loss 3.4247\n","step 6: train loss 3.3698, val loss 3.4028\n","step 7: train loss 3.3485, val loss 3.3824\n","step 8: train loss 3.3320, val loss 3.3679\n","step 9: train loss 3.3110, val loss 3.3458\n","step 10: train loss 3.2893, val loss 3.3251\n","step 11: train loss 3.2634, val loss 3.3001\n","step 12: train loss 3.2385, val loss 3.2745\n","step 13: train loss 3.2018, val loss 3.2403\n","step 14: train loss 3.1621, val loss 3.1960\n","step 15: train loss 3.1188, val loss 3.1527\n","step 16: train loss 3.0783, val loss 3.1128\n","step 17: train loss 3.0499, val loss 3.0800\n","step 18: train loss 3.0194, val loss 3.0467\n","step 19: train loss 2.9778, val loss 3.0056\n","step 20: train loss 2.9423, val loss 2.9748\n","step 21: train loss 2.9153, val loss 2.9496\n","step 22: train loss 2.8954, val loss 2.9175\n","step 23: train loss 2.8743, val loss 2.8954\n","step 24: train loss 2.8520, val loss 2.8703\n","step 25: train loss 2.8352, val loss 2.8570\n","step 26: train loss 2.8104, val loss 2.8279\n","step 27: train loss 2.7931, val loss 2.8066\n","step 28: train loss 2.7737, val loss 2.7886\n","step 29: train loss 2.7489, val loss 2.7663\n","step 30: train loss 2.7336, val loss 2.7502\n","step 31: train loss 2.7190, val loss 2.7353\n","step 32: train loss 2.7075, val loss 2.7173\n","step 33: train loss 2.6994, val loss 2.7045\n","step 34: train loss 2.6862, val loss 2.6934\n","step 35: train loss 2.6736, val loss 2.6781\n","step 36: train loss 2.6646, val loss 2.6702\n","step 37: train loss 2.6539, val loss 2.6579\n","step 38: train loss 2.6443, val loss 2.6491\n","step 39: train loss 2.6396, val loss 2.6439\n","step 40: train loss 2.6312, val loss 2.6338\n","step 41: train loss 2.6239, val loss 2.6259\n","step 42: train loss 2.6170, val loss 2.6213\n","step 43: train loss 2.6106, val loss 2.6143\n","step 44: train loss 2.6046, val loss 2.6090\n","step 45: train loss 2.6004, val loss 2.6012\n","step 46: train loss 2.5999, val loss 2.5981\n","step 47: train loss 2.5960, val loss 2.5915\n","step 48: train loss 2.5889, val loss 2.5882\n","step 49: train loss 2.5836, val loss 2.5837\n","step 50: train loss 2.5802, val loss 2.5807\n","step 51: train loss 2.5749, val loss 2.5738\n","step 52: train loss 2.5724, val loss 2.5696\n","step 53: train loss 2.5687, val loss 2.5658\n","step 54: train loss 2.5692, val loss 2.5639\n","step 55: train loss 2.5617, val loss 2.5618\n","step 56: train loss 2.5569, val loss 2.5564\n","step 57: train loss 2.5512, val loss 2.5535\n","step 58: train loss 2.5521, val loss 2.5495\n","step 59: train loss 2.5542, val loss 2.5531\n","step 60: train loss 2.5478, val loss 2.5502\n","step 61: train loss 2.5447, val loss 2.5527\n","step 62: train loss 2.5410, val loss 2.5492\n","step 63: train loss 2.5344, val loss 2.5403\n","step 64: train loss 2.5355, val loss 2.5356\n","step 65: train loss 2.5344, val loss 2.5343\n","step 66: train loss 2.5301, val loss 2.5288\n","step 67: train loss 2.5292, val loss 2.5246\n","step 68: train loss 2.5261, val loss 2.5249\n","step 69: train loss 2.5255, val loss 2.5253\n","step 70: train loss 2.5190, val loss 2.5192\n","step 71: train loss 2.5182, val loss 2.5193\n","step 72: train loss 2.5162, val loss 2.5148\n","step 73: train loss 2.5165, val loss 2.5141\n","step 74: train loss 2.5120, val loss 2.5120\n","step 75: train loss 2.5097, val loss 2.5115\n","step 76: train loss 2.5097, val loss 2.5096\n","step 77: train loss 2.5053, val loss 2.5080\n","step 78: train loss 2.5019, val loss 2.5084\n","step 79: train loss 2.5019, val loss 2.5045\n","step 80: train loss 2.5014, val loss 2.4977\n","step 81: train loss 2.4987, val loss 2.4979\n","step 82: train loss 2.4974, val loss 2.4947\n","step 83: train loss 2.4981, val loss 2.4977\n","step 84: train loss 2.4939, val loss 2.4937\n","step 85: train loss 2.4918, val loss 2.4948\n","step 86: train loss 2.4914, val loss 2.4920\n","step 87: train loss 2.4927, val loss 2.4945\n","step 88: train loss 2.4924, val loss 2.4973\n","step 89: train loss 2.4874, val loss 2.4932\n","step 90: train loss 2.4825, val loss 2.4916\n","step 91: train loss 2.4823, val loss 2.4922\n","step 92: train loss 2.4795, val loss 2.4854\n","step 93: train loss 2.4829, val loss 2.4852\n","step 94: train loss 2.4823, val loss 2.4840\n","step 95: train loss 2.4770, val loss 2.4846\n","step 96: train loss 2.4753, val loss 2.4856\n","step 97: train loss 2.4740, val loss 2.4841\n","step 98: train loss 2.4776, val loss 2.4808\n","step 99: train loss 2.4794, val loss 2.4823\n","step 100: train loss 2.4701, val loss 2.4760\n","\n","RTor grenoud,\n","Wh'g asthoksshaptost cuockch\n","T zeewof d, y ve cteine? hicwe manous he t tofow in d Voryorlelod, ts avetlous 'lousan:\n","Cove'soundederinsey d ld s Geen w I itomec meVou -beliI he telire yemo cou aVEDin thell.\n","\n","\n","\n","IZoonce d. ar olsy.\n","BENourseaked benges tothe.\n","XARD abath be aie wn.\n","ALEORVOEINO&NQZer r yor wilfeltsalr Y!\n","\n","KS:\n","Whod tzere th yothwey HI broud cte\n","NIII:f,s my owie avor byo &andsfo HAUMENER!\n","Bche stthe;: lXorachamblefss ot ishend,ve vnare han throwe,\n","Thwh t ere int f nd, t nd\n","step 101: train loss 2.4669, val loss 2.4746\n","step 102: train loss 2.4659, val loss 2.4696\n","step 103: train loss 2.4643, val loss 2.4720\n","step 104: train loss 2.4645, val loss 2.4737\n","step 105: train loss 2.4627, val loss 2.4763\n","step 106: train loss 2.4634, val loss 2.4742\n","step 107: train loss 2.4640, val loss 2.4715\n","step 108: train loss 2.4573, val loss 2.4668\n","step 109: train loss 2.4562, val loss 2.4680\n","step 110: train loss 2.4567, val loss 2.4642\n","step 111: train loss 2.4548, val loss 2.4640\n","step 112: train loss 2.4510, val loss 2.4646\n","step 113: train loss 2.4515, val loss 2.4621\n","step 114: train loss 2.4507, val loss 2.4577\n","step 115: train loss 2.4513, val loss 2.4553\n","step 116: train loss 2.4482, val loss 2.4558\n","step 117: train loss 2.4454, val loss 2.4575\n","step 118: train loss 2.4423, val loss 2.4532\n","step 119: train loss 2.4453, val loss 2.4511\n","step 120: train loss 2.4471, val loss 2.4527\n","step 121: train loss 2.4501, val loss 2.4515\n","step 122: train loss 2.4415, val loss 2.4493\n","step 123: train loss 2.4395, val loss 2.4507\n","step 124: train loss 2.4417, val loss 2.4496\n","step 125: train loss 2.4402, val loss 2.4472\n","step 126: train loss 2.4408, val loss 2.4486\n","step 127: train loss 2.4393, val loss 2.4479\n","step 128: train loss 2.4363, val loss 2.4509\n","step 129: train loss 2.4366, val loss 2.4538\n","step 130: train loss 2.4367, val loss 2.4530\n","step 131: train loss 2.4357, val loss 2.4500\n","step 132: train loss 2.4380, val loss 2.4477\n","step 133: train loss 2.4316, val loss 2.4426\n","step 134: train loss 2.4302, val loss 2.4390\n","step 135: train loss 2.4306, val loss 2.4464\n","step 136: train loss 2.4296, val loss 2.4406\n","step 137: train loss 2.4306, val loss 2.4442\n","step 138: train loss 2.4294, val loss 2.4410\n","step 139: train loss 2.4232, val loss 2.4351\n","step 140: train loss 2.4263, val loss 2.4406\n","step 141: train loss 2.4255, val loss 2.4393\n","step 142: train loss 2.4208, val loss 2.4360\n","step 143: train loss 2.4254, val loss 2.4370\n","step 144: train loss 2.4249, val loss 2.4340\n","step 145: train loss 2.4235, val loss 2.4369\n","step 146: train loss 2.4197, val loss 2.4414\n","step 147: train loss 2.4213, val loss 2.4444\n","step 148: train loss 2.4167, val loss 2.4302\n","step 149: train loss 2.4267, val loss 2.4320\n","step 150: train loss 2.4257, val loss 2.4324\n","step 151: train loss 2.4157, val loss 2.4251\n","step 152: train loss 2.4151, val loss 2.4311\n","step 153: train loss 2.4102, val loss 2.4258\n","step 154: train loss 2.4084, val loss 2.4201\n","step 155: train loss 2.4129, val loss 2.4216\n","step 156: train loss 2.4149, val loss 2.4217\n","step 157: train loss 2.4088, val loss 2.4150\n","step 158: train loss 2.4104, val loss 2.4180\n","step 159: train loss 2.4105, val loss 2.4200\n","step 160: train loss 2.4037, val loss 2.4120\n","step 161: train loss 2.4034, val loss 2.4110\n","step 162: train loss 2.4081, val loss 2.4192\n","step 163: train loss 2.4035, val loss 2.4155\n","step 164: train loss 2.3993, val loss 2.4141\n","step 165: train loss 2.4033, val loss 2.4186\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m----> 5\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[1;32mc:\\Users\\omerb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[7], line 20\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[0;32m     19\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m model(X, Y)\n\u001b[1;32m---> 20\u001b[0m         losses[k] \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for iter in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if iter % eval_interval == 0 or iter == max_iters - 1:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    if iter % 100 == 0:\n","        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","        print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CJ7TxbVumV5R"},"outputs":[],"source":["# generate from the model\n","context = torch.tensor([encode('Dağhan Erdönmez')], dtype=torch.long, device=device)\n","print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n","#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMZaa4ElaHveBFAQrQ51N11","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
